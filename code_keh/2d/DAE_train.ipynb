{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55219cd38a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;31m#类\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msqueezenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'AlexNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alexnet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/model_zoo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrequests_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__copyright__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__cake__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcerts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# to_native_string is unused here, but imported here for backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_native_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_http_list\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parse_list_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m from .compat import (\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/_internal_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_py2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuiltin_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlunparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munquote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munquote_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murldefrag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_http_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_bypass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_bypass_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetproxies_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcookiejar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcookielib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorsel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/cookiejar.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m     r\"(\\d\\d\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d) GMT$\", re.ASCII)\n\u001b[1;32m    204\u001b[0m WEEKDAY_RE = re.compile(\n\u001b[0;32m--> 205\u001b[0;31m     r\"^(?:Sun|Mon|Tue|Wed|Thu|Fri|Sat)[a-z]*,?\\s*\", re.I | re.ASCII)\n\u001b[0m\u001b[1;32m    206\u001b[0m LOOSE_HTTP_DATE_RE = re.compile(\n\u001b[1;32m    207\u001b[0m     r\"\"\"^\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;31m# print(code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36m_code\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;31m# compile the pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUCCESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, pattern, flags)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUCCESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mcode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, pattern, flags)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mSUCCESS_CODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SUCCESS_CODES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mASSERT_CODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ASSERT_CODES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     if (flags & SRE_FLAG_IGNORECASE and\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_LOCALE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_UNICODE\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__xor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;31m# see if it's in the reverse mapping (for hashable values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value2member_map_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value2member_map_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun 16 18:11:40 2020\n",
    "\n",
    "@author: 12057\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(r'/home/sufedc_nvidia_newgyh/apex')\n",
    "from apex import amp\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn #类\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from config import Config #获取配置\n",
    "\n",
    "\n",
    "class Denoising_AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    autoencoder神经网络结构搭建\n",
    "    \"\"\"\n",
    "    cfg_conv = [[3, 64, 3, 1, 1], [64, 64, 3, 1, 1], 'M', [64, 128, 3, 1, 1], [128, 128, 3, 1, 1], 'M',\n",
    "                 [128, 256, 3, 1, 1], [256, 256, 3, 1, 1], [256, 256, 3, 1, 1], [256, 256, 3, 1, 1], 'M', \n",
    "                 [256, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], 'M', \n",
    "                 [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], 'M']\n",
    "    #encode网络架构图  conv:[inner_channels, outer_channels, filter, stride, padding]\n",
    "                      #最大池化层:M  2*2 步长2\n",
    "\n",
    "    cfg_tranconv = ['M', [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1],\n",
    "                    'M', [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 512, 3, 1, 1], [512, 256, 3, 1, 1],  \n",
    "                    'M', [256, 256, 3, 1, 1], [256, 256, 3, 1, 1], [256, 256, 3, 1, 1],  [256, 128, 3, 1, 1],\n",
    "                    'M', [128, 128, 3, 1, 1], [128, 64, 3, 1, 1], 'M', [64, 64, 3, 1, 1], [64, 3, 3, 1, 1]]     #decode网络架构图  tran_conv:[inner_channels, outer_channels, filter, stride, padding]\n",
    "                     #最大逆池化层:M  2*2 步长2\n",
    "\n",
    "    pool_kernel_size = 2 #池化层核大小\n",
    "    pool_stride = 2 #池化层步长\n",
    "\n",
    "    pool_index_list = [] #池化原始所在位置list\n",
    "    \n",
    "    def __init__(self, feature_len, img_size):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param: feature_len encode长度\n",
    "        :param: img_size 图片尺寸           \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Denoising_AutoEncoder, self).__init__() #等价与nn.Module.__init__()   运用nn.Module初始化\n",
    "        \n",
    "        self.img_size = img_size #图片大小\n",
    "        self.feature_len = feature_len #encode长度\n",
    "        self.in_channels = 3 #记录输入层数，用于网络输入\n",
    "        self.en_net = self.encode_conv() #encode网络\n",
    "        self.en_fc = nn.Linear(in_features = self.in_channels * self.img_size[0] * self.img_size[1], out_features = self.feature_len, bias = True) #encode最后的全连接层       \n",
    "        self.de_fc = nn.Linear(in_features = self.feature_len, out_features = self.in_channels * self.img_size[0] * self.img_size[1], bias = True) #decode最初的全连接层     \n",
    "        self.de_net = self.decode_conv() #decode网络     \n",
    "\n",
    "    def corrupt_x(self, x, cor_rate = 0.01):\n",
    "        \"\"\"\n",
    "        对于x进行随机corrupt\n",
    "        :param: x 输入变量x\n",
    "        :param: cor_rate 随机corrupt概率\n",
    "        return corrupted_x corrupt后的变量x\n",
    "        \"\"\"\n",
    "        judge_matrix = np.random.randn(*list(x.shape)) > cor_rate\n",
    "        judge_matrix = judge_matrix.astype('float32')\n",
    "        judge_matrix = torch.FloatTensor(judge_matrix)\n",
    "        try: #用gpu\n",
    "            judge_matrix = judge_matrix.cuda(x.device)\n",
    "        except: #用cpu\n",
    "            pass\n",
    "        corrupted_x = x * judge_matrix\n",
    "        \n",
    "        return corrupted_x\n",
    "    \n",
    "    def encode_conv(self):\n",
    "        \"\"\"\n",
    "        建立encode部分网络, 不用ReLU稀疏特征, 不用池化层\n",
    "        return  encode_feature_net  encode部分\n",
    "        \"\"\"\n",
    "        encode_feature_net = []\n",
    "        for layer_type in self.cfg_conv:\n",
    "            if layer_type == 'M':\n",
    "                layer = [nn.MaxPool2d(kernel_size = self.pool_kernel_size, stride = self.pool_stride, return_indices = True)]#最大池化层\n",
    "                        #对应原始所在位置 \n",
    "                self.img_size[0] = int((self.img_size[0] - self.pool_kernel_size) / self.pool_stride + 1) #计算图片大小\n",
    "                self.img_size[1] = int((self.img_size[1] - self.pool_kernel_size) / self.pool_stride + 1)#计算图片大小\n",
    "            else: #卷积层\n",
    "                in_channels, out_channels, kernel_size, stride, padding = layer_type\n",
    "                self.img_size[0] = int((self.img_size[0] - kernel_size + 2 * padding) / stride + 1) #计算图片大小\n",
    "                self.img_size[1] = int((self.img_size[1] - kernel_size + 2 * padding) / stride + 1)#计算图片大小\n",
    "                layer = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding), #卷积层\n",
    "                                  #输入通道数     输出特征数    卷积核大小    步长    填充\n",
    "                         nn.BatchNorm2d(num_features = out_channels), #BN层\n",
    "                                             #特征数\n",
    "                         nn.Tanh()]  #Tanh层 \n",
    "                         #inplace=True改变输入的数据, 节省内存\n",
    "                self.in_channels = out_channels\n",
    "                \n",
    "            encode_feature_net += layer\n",
    "                  \n",
    "        return nn.Sequential(*encode_feature_net)\n",
    "    \n",
    "    \n",
    "    def decode_conv(self):\n",
    "        \"\"\"\n",
    "        建立decode部分网络, 不用ReLU稀疏特征, 不用逆池化层\n",
    "        return  decode_pic_net  encode部分\n",
    "        \"\"\"\n",
    "        decode_pic_net = []\n",
    "        for layer_type in self.cfg_tranconv:\n",
    "            if layer_type == 'M':\n",
    "                layer = [nn.MaxUnpool2d(kernel_size = self.pool_kernel_size, stride = self.pool_stride)]#最大逆池化层\n",
    "            else: #卷积层\n",
    "                in_channels, out_channels, kernel_size, stride, padding = layer_type\n",
    "                layer = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding), #卷积层\n",
    "                                  #输入通道数     输出特征数    卷积核大小    步长    填充\n",
    "                         nn.BatchNorm2d(num_features = out_channels), #BN层\n",
    "                                             #特征数\n",
    "                         nn.Tanh()]  #Tanh层 \n",
    "                         #inplace=True改变输入的数据, 节省内存\n",
    "                #self.in_channels = out_channels\n",
    "                \n",
    "            decode_pic_net += layer\n",
    "        decode_pic_net[-1] = nn.Sigmoid() #最后一层改成Sigmoid 让输入为0-1之间\n",
    "            \n",
    "        return nn.Sequential(*decode_pic_net)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param: x 图片变量\n",
    "        return code: 图片encode编码 \n",
    "               decode: encode编码 decode结果\n",
    "        \"\"\"\n",
    "        \n",
    "        code = self.corrupt_x(x) #corrupt_x\n",
    "        for net in self.en_net: #encode部分\n",
    "            if isinstance(net, nn.MaxPool2d):\n",
    "                code, pool_index = net(code)  #获取池化位置\n",
    "                self.pool_index_list.append(pool_index)\n",
    "            else:\n",
    "                code = net(code)\n",
    "                \n",
    "        code = code.view(code.size(0), -1)\n",
    "        code = self.en_fc(code)\n",
    "        \n",
    "        decode = self.de_fc(code)\n",
    "        decode = decode.view(decode.size(0), self.in_channels, self.img_size[0], self.img_size[1])\n",
    "        \n",
    "        for net in self.de_net: #decode部分\n",
    "            if isinstance(net, nn.MaxUnpool2d):\n",
    "                decode = net(decode, self.pool_index_list.pop())\n",
    "            else:\n",
    "                decode = net(decode)\n",
    "        \n",
    "        return code, decode\n",
    "\n",
    "    def encode_data(self, x):\n",
    "        code = x \n",
    "        for net in self.en_net: #encode部分\n",
    "            if isinstance(net, nn.MaxPool2d):\n",
    "                code, pool_index = net(code)  #获取池化位置\n",
    "                self.pool_index_list.append(pool_index)\n",
    "            else:\n",
    "                code = net(code)\n",
    "                \n",
    "        code = code.view(code.size(0), -1)\n",
    "        code = self.en_fc(code)\n",
    "        \n",
    "        return code\n",
    "    \n",
    "def get_data(cfg):\n",
    "    \"\"\"\n",
    "    获取预处理和数据增强后的数据集\n",
    "    :param: cfg 配置文件\n",
    "    return trainloader, testloader, data_ok\n",
    "           训练loader   测试loader  数据是否获得\n",
    "    \"\"\"\n",
    "    print('>' * 80)\n",
    "    print('Date getting begin')\n",
    "    print('')\n",
    "    \n",
    "    try: \n",
    "        #训练集数据获取\n",
    "        \n",
    "        #训练集数据预处理\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(size = cfg.resize_size, interpolation = PIL.Image.BILINEAR), #调整大小一致，所有图片大小需要一致\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        trainset = torchvision.datasets.ImageFolder(os.path.join(cfg.data_info, 'train'), transform = transform_train)\n",
    "        \n",
    "        #testset = torchvision.datasets.ImageFolder(os.path.join(cfg.data_info, 'test',), transform = transform_train)\n",
    "                      \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size = cfg.batch_size, shuffle = True, num_workers = 0)\n",
    "\n",
    "        testloader = torch.utils.data.DataLoader(trainset, batch_size = 1, shuffle = False, num_workers = 0)\n",
    "\n",
    "        print('Succeeded to get_data')\n",
    "        print('>' * 80)\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        \n",
    "        return trainloader, testloader, True \n",
    "    \n",
    "    except: #防爆\n",
    "        \n",
    "        print('Failed to get_data, stop training')\n",
    "        print('>' * 80)\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        \n",
    "        return None, None, False\n",
    "    \n",
    "\"\"\"\n",
    "通用模型初始化\n",
    "\"\"\"\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    模型初始化\n",
    "    :param:model 输入模型 可以用model.apply(initialize_weights)调用\n",
    "    \"\"\"\n",
    "    for module in model.modules(): #模型中的所有模式，包含总，序列，层\n",
    "        \n",
    "        if isinstance(module, nn.Conv2d): #卷积层\n",
    "            n_conv = module.kernel_size[0] * module.kernel_size[1] * module.out_channels #卷积权重元素个数\n",
    "            module.weight.data.normal_(0, math.sqrt(2. / n_conv)) #卷积核初始化 正态随机数, 限制标准差\n",
    "            if module.bias is not None: #有偏移项\n",
    "                module.bias.data.zero_() #偏移项初始化 = 0\n",
    "                \n",
    "        elif isinstance(module, nn.BatchNorm2d): #BN层\n",
    "            module.weight.data.fill_(1) #归一化权重 == 标准差, 初始化 = 1\n",
    "            module.bias.data.zero_() #归一化偏移项 == 均值, 初始化 = 0\n",
    "            \n",
    "        elif isinstance(module, nn.Linear): #全连接层\n",
    "            n_fc = module.in_features * module.out_features #全连接层权重个数\n",
    "            module.weight.data.normal_(0, math.sqrt(2. / n_fc)) #全连接权重正态初始化\n",
    "            module.bias.data.zero_()#偏移项初始化 = 0\n",
    "            \n",
    "        elif isinstance(module, nn.ConvTranspose2d): #反卷积层\n",
    "            n_conv = module.kernel_size[0] * module.kernel_size[1] * module.out_channels #卷积权重元素个数\n",
    "            module.weight.data.normal_(0, math.sqrt(2. / n_conv)) #卷积核初始化 正态随机数, 限制标准差\n",
    "            if module.bias is not None: #有偏移项\n",
    "                module.bias.data.zero_() #偏移项初始化 = 0\n",
    "                \n",
    "#模型训练\n",
    "def train(trainloader, cfg):\n",
    "    \"\"\"\n",
    "    :param: trainloader 训练数据loader\n",
    "    :param: cfg 配置文件\n",
    "    return train_loss 各epoch训练损失函数list\n",
    "    \"\"\"\n",
    "    print('>' * 80)    \n",
    "    print('Begin train')\n",
    "    print(' ')\n",
    "\n",
    "    #模型基本配置\n",
    "    print('Model use {}'.format(cfg.model_name))\n",
    "    print(' ')\n",
    "    \n",
    "    model = Denoising_AutoEncoder(cfg.classes_num, list(cfg.resize_size)) #载入模型\n",
    "\n",
    "    begin_epoch = 0 #初始epoch\n",
    "    if cfg.pretrain: #如果有预训练\n",
    "        model_info = torch.load(os.path.join(cfg.pretrain, 'model_{}_state.pkl'.format(cfg.model_name)))\n",
    "        model.load_state_dict(model_info['state_dict']) #加载训练出的模型\n",
    "        begin_epoch = model_info['epoch'] + 1\n",
    "\n",
    "    model.train() #切换到训练模式\n",
    "          \n",
    "    device = torch.device(cfg.device) #选择设备\n",
    "    try: #有gpu\n",
    "        model.cuda(device) #设备选择\n",
    "        gpu = True #是否有gpu\n",
    "        print('Gpu is used')\n",
    "    except:#不用gpu\n",
    "        gpu = False\n",
    "        print('Cpu is used')\n",
    "    \n",
    "    criterion = torch.nn.MSELoss() #损失函数方法：MSE\n",
    "    alpha = cfg.alpha if not cfg.pretrain else model_info['alpha'] #初始学习率，有预处理以预处理为准\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum = cfg.momentum, lr = alpha, weight_decay = cfg.weight_decay) #迭代方法SGD\n",
    "                                                           #动量              初始学习率            权重衰减趋势\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-8, weight_decay = cfg.weight_decay) #迭代方法Adam\n",
    "                                                       #学习率      梯度及梯度平方系数   分母防零修正           权重衰减系数\n",
    "                                                       \n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10) #学习率衰减(余弦退火)\n",
    "                                                        #0, T_max下降，T_max到2 * T_max上升\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda = lambda step:np.sin(step) / step) #自己设定,函数输入为步数\n",
    "                                                                    #自己设定函数\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [20, 80], gamma = 0.9) #分段式衰减\n",
    "                                                                #设定变化点，遇到该点变化  衰减系数\n",
    "    #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.99) #指数衰减，每个epoch\n",
    "                                                                   #衰减系数\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10, verbose = False, threshold = 0.0001, threshold_mode = 'rel', cooldown = 0, min_lr = 0, eps = 1e-08) #自适应\n",
    "                                                                     #检测loss减小     衰减系数       容忍次数        是否print         变化阈值范围        rel比例 abs值           冷却时间      最小lr     效果较差不变                                  \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = cfg.lr_step, gamma = cfg.lr_decay) #学习率线性衰减\n",
    "                                                                 #衰减步长         衰减系数 lr *= lr_decay\n",
    "    if not cfg.pretrain: #如果没预训练\n",
    "        model.apply(initialize_weights) #模型初始化，内置初始化，均匀分布\n",
    "    \n",
    "    if gpu:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level = \"O1\", verbosity = 0) #混合精度模型\n",
    "                                              #Oo fp32, O1混合, O2几乎fp16, O3 fp16  \n",
    "    if cfg.distribution:\n",
    "        model = torch.nn.DataParallel(model, device_ids = list(range(cfg.distribution)))\n",
    "        \n",
    "    if cfg.pretrain: #如果预训练\n",
    "        train_loss = model_info['train_loss']\n",
    "    else:\n",
    "        train_loss = []\n",
    "    \n",
    "    total_start_time = time.time() #记录时间\n",
    "    start_time = time.time() #记录时间\n",
    "    \n",
    "    for epoch in tqdm(range(begin_epoch, cfg.epoch_num)): #迭代全图\n",
    "    #for epoch in range(1, cfg.epoch_num + 1): #迭代全图  \n",
    "        \n",
    "        train_loss_i = 0 #第i次epoch损失\n",
    "        for batch_idx, (imgs, _) in enumerate(trainloader): #迭代批次\n",
    "            #批数       图片\n",
    "            \n",
    "            #one-hot label 化：(交叉熵里面自动有)\n",
    "            #classes = torch.zeros(cfg.batch_size, len(cfg.classes)).scatter_(1, classes.view(len(classes),1), 1)\n",
    "                                                                     #稀疏化 维度       值                  对应标签值           \n",
    "            if gpu: #用gpu\n",
    "                imgs = imgs.cuda(device) #将数据移到GPU上\n",
    "                inputs= Variable(imgs)  #变量化输入x,y\n",
    "         \n",
    "            optimizer.zero_grad()   # 先将optimizer梯度先置为0\n",
    "            \n",
    "            encode, decode = model(inputs) #前向传播\n",
    "            #outputs = model.forward(inputs) #等价效果\n",
    "            \n",
    "            loss = criterion(inputs, decode) #损失函数\n",
    "\n",
    "            if gpu:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:#采用混合精度模型         \n",
    "                    scaled_loss.backward() \n",
    "            else:\n",
    "                loss.backward()  #反向传播，计算梯度\n",
    "            \n",
    "            optimizer.step() #更新参数\n",
    "        \n",
    "            train_loss_i += loss.data.item()#记录每次训练Loss, 必须loss.data[0]\n",
    "            \n",
    "        \n",
    "        scheduler.step() #学习率记录step      \n",
    "        train_loss.append(train_loss_i) #记录每轮的损失函数值\n",
    "        \n",
    "        if  epoch % 5 == 4: #每十次迭代            \n",
    "            end_time = time.time() #记录时间\n",
    "            \n",
    "            #展示模型训练状态\n",
    "            print(' ')\n",
    "            print('>' * 80)    \n",
    "            print('Epoch : {} - {}'.format(epoch - 3, epoch + 1))\n",
    "            print('Training_time = {} s / epoch'.format(str( (end_time - start_time) / 5 )[:8]) )\n",
    "            print('Avg_loss_function = {}'.format(np.mean(train_loss[-5:])))\n",
    "            print('>' * 80)\n",
    "            print(' ')\n",
    "            \n",
    "            if not cfg.mix_up and epoch / cfg.epoch_num > 0.1: #预热10%迭代\n",
    "                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), \n",
    "                            'train_loss': train_loss, 'alpha': optimizer.state_dict()['param_groups'][0]['lr']}, #记录迭代次数，状态字典，最好结果, 损失函数list, 学习率\n",
    "                            os.path.join(cfg.save_path, 'model_{}_state.pkl'.format(cfg.model_name))) #最好的结果(覆盖原来的)\n",
    "                \n",
    "                #torch.save(model.state_dict(), os.path.join(cfg.save_path, 'model_{}_state.pkl'.format(cfg.model_name))) #最好的结果(覆盖原来的)\n",
    "                #保存中间最好的模型(以后可以再训练) 保存模型所有信息，读取时要载入框架\n",
    "                #torch.save(model, './model_{}.pkl'.format(cfg.model_name)) #保存模型信息，读取时直接读取 等价\n",
    "\n",
    "            start_time = time.time() #更新时间\n",
    "            \n",
    "        torch.cuda.empty_cache() #清理显存\n",
    "         \n",
    "    total_end_time = time.time() #记录时间           \n",
    "    \n",
    "    print('Training time = {} s / epoch'.format( str( (total_end_time - total_start_time) / cfg.epoch_num )[:8] ) )\n",
    "    print(' ')\n",
    "    print('Finish train')\n",
    "    print('>' * 80)    \n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "    \n",
    "#模型测试\n",
    "def test(testloader, cfg):\n",
    "    \"\"\"\n",
    "    测试Auto_encoder效果\n",
    "    :param: testloader 测试数据loader\n",
    "    :param: cfg 配置文件\n",
    "    return: precision 准确率\n",
    "    \"\"\"\n",
    "    print('>' * 80)\n",
    "    print('Begin test')\n",
    "    print(' ')\n",
    "    \n",
    "    dic_class = {0: 'xinguan', 1: 'noxinguan'}\n",
    "    if not os.path.exists(os.path.join(cfg.save_path, 'dae_result')):\n",
    "        os.mkdir(os.path.join(cfg.save_path, 'dae_result'))\n",
    "        for key in dic_class:\n",
    "            os.mkdir(os.path.join(cfg.save_path, 'dae_result', dic_class[key]))\n",
    "        \n",
    "    #载入模型结构   \n",
    "    model = Denoising_AutoEncoder(cfg.classes_num, list(cfg.resize_size)) #载入模型\n",
    "    \n",
    "    model_info = torch.load(os.path.join(cfg.save_path, 'model_{}_state.pkl'.format(cfg.model_name))) #获取字典\n",
    "    model.load_state_dict(model_info['state_dict']) #加载训练出的模型\n",
    "    \n",
    "    #model.load_state_dict(torch.load(os.path.join(cfg.save_path, 'model_{}_state.pkl'.format(cfg.model_name)))) #加载训练出的模型\n",
    "    #model = torch.load(r'./model_{}.pkl'.format(cfg.model_name)) 等价\n",
    "    \n",
    "    model.eval() #测试，不改变权重\n",
    "\n",
    "    device = torch.device(cfg.device) #选择设备\n",
    "    try: #用gpu\n",
    "        model.cuda(device) #设备选择\n",
    "        gpu = True #是否有gpu\n",
    "        print('Gpu is used')\n",
    "    except:#不用gpu\n",
    "        gpu = False\n",
    "        print('Cpu is used')\n",
    "    \n",
    "    total = 0 #图片总数\n",
    "\n",
    "    with torch.no_grad(): #不进行反向传播, 减少内存\n",
    "    \n",
    "        start_time = time.time() #记录时间\n",
    "        \n",
    "        for idx, (imgs, classes) in enumerate(testloader): #遍历图片\n",
    "           #索引  图片  \n",
    "           if gpu:#用gpu\n",
    "               imgs= imgs.cuda(device)   # 将数据移到GPU上\n",
    "           inputs = Variable(imgs) #变量化输入x\n",
    "           \n",
    "           encode, decode = model(inputs) #运行模型(获得结果)\n",
    "           \n",
    "           encode_pic = encode.cpu().numpy()[0, :]\n",
    "           classes = int(classes.cpu().numpy())\n",
    "           #print(classes)\n",
    "           encode_size = int(np.sqrt(len(encode_pic)))\n",
    "           encode_pic = np.reshape(encode_pic, (encode_size, encode_size))\n",
    "           np.save(os.path.join(cfg.save_path, 'dae_result', dic_class[classes], str(idx) + '.npy'), encode_pic)\n",
    "           #with open(os.path.join(cfg.save_path, 'dae_result', dic_class[classes], str(idx) + '.pkl'), 'wb') as pkl:\n",
    "                #pickle.dump(encode_pic, pkl)\n",
    "\n",
    "           #dec_pic = transforms.ToPILImage()(decode.cpu()[0, :, :, :])       \n",
    "           #dec_pic.save(os.path.join(cfg.save_path, 'ae_result', str(idx) + '.png')) #等价方法\n",
    "           \n",
    "           total += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "             \n",
    "        torch.cuda.empty_cache() #清理显存\n",
    "    \n",
    "    print('Encoding time = {} s / pic'.format( str( (end_time - start_time) / total )[:8] ) )\n",
    "    print(' ')\n",
    "    print('Finish test')\n",
    "    print('>' * 80)    \n",
    "    print(' ')\n",
    "    print(' ')\n",
    "        \n",
    "    \n",
    "    \n",
    "def main(config_path = r'./config_ae.yml'):\n",
    "    \"\"\"\n",
    "    主函数, 完成数据载入和模型训练 + 测试\n",
    "    :param: config_path config路径\n",
    "    return: train_loss,        precision,       wronglist       \n",
    "            各epcoh下训练损失   测试准确性       错判断序号列表\n",
    "    \"\"\"\n",
    "    cfg = Config(config_path) #获取配置文件\n",
    "    trainloader, testloader, data_ok = get_data(cfg) #获取数据并进行数据预处理和增强\n",
    "    if data_ok:\n",
    "        train_loss = train(trainloader, cfg) #模型训练\n",
    "        test(testloader, cfg) #模型训练\n",
    "        return train_loss\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    train_loss = main(r'./config_dae.yml')\n",
    "    while True:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,2,3)\n",
    "import torch\n",
    "torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
